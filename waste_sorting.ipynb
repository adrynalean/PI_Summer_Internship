{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1vFLg-jc9ktSGXmLMsmkJmyHi8AzoVMLg",
      "authorship_tag": "ABX9TyMbwLVp2k0smUiMkcqw4o90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrynalean/PI_Summer_Internship/blob/prototype_v1_CNN_Module/waste_sorting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Food Waste Sorting Machine"
      ],
      "metadata": {
        "id": "R8KvP6JC7UCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Accumulating data"
      ],
      "metadata": {
        "id": "eyQzQttQ7YjM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZIuIN_Ct7EnG"
      },
      "outputs": [],
      "source": [
        "# # Uploading the json file from kaggle\n",
        "\n",
        "# from google.colab import files\n",
        "# files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INstalling kaggle API and configuring it\n",
        "\n",
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle/\n",
        "!cp \"/content/drive/MyDrive/Food_Waste_project/kaggle.json\" ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck_zMY5r77LE",
        "outputId": "0af2481c-f510-4278-d985-606b54b8d84b",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the dataset from kaggle\n",
        "!kaggle datasets download -d joebeachcapital/realwaste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZEI80Lv9SKk",
        "outputId": "70eb0bef-d980-4d05-b8b6-ea65b6029b95"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/joebeachcapital/realwaste\n",
            "License(s): Attribution 4.0 International (CC BY 4.0)\n",
            "Downloading realwaste.zip to /content\n",
            " 99% 652M/657M [00:07<00:00, 85.1MB/s]\n",
            "100% 657M/657M [00:07<00:00, 95.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the dataset\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "zip_path = \"/content/realwaste.zip\"\n",
        "extract_dir = \"/content/realwaste\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "  zip_ref.extractall(extract_dir)"
      ],
      "metadata": {
        "id": "wNCRtjct91GI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the dataset directory structure\n",
        "\n",
        "os.listdir(extract_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WoJbfzG92k9",
        "outputId": "f639fe27-d068-4e30-83d5-b84ddde088a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['realwaste-main']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define data transformations and data loaders"
      ],
      "metadata": {
        "id": "7dTczqLE-gdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# define data transformations\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "}\n",
        "\n",
        "\n",
        "# Set data directories\n",
        "data_dir = '/content/realwaste/realwaste-main/RealWaste'\n"
      ],
      "metadata": {
        "id": "56xjlFBj-gCn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating train and test split\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Create dataset\n",
        "dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms['train'])\n",
        "\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "test_size = total_size - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n"
      ],
      "metadata": {
        "id": "ilM2xzsHAGyp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "\n",
        "class_names = dataset.classes\n",
        "\n",
        "print(f\"Classes: {class_names}\")\n",
        "print(f\"Dataset Sizes: Train: {train_size}, Test: {test_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6pT7QnHAcEp",
        "outputId": "4c082526-6564-4a78-fa36-a177c5c62479"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['Cardboard', 'Food Organics', 'Glass', 'Metal', 'Miscellaneous Trash', 'Paper', 'Plastic', 'Textile Trash', 'Vegetation']\n",
            "Dataset Sizes: Train: 3801, Test: 951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Build the Model"
      ],
      "metadata": {
        "id": "cC0x11GTC70_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "fk7ODHdTC-Wv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the model\n",
        "\n",
        "class wasteCNN(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(wasteCNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 32, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding= 1)\n",
        "    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride = 2, padding=0)\n",
        "    self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
        "    self.fc2 = nn.Linear( 512, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = self.pool(F.relu(self.conv3(x)))\n",
        "    x = x.view(-1, 128 * 28 *28) # flatten the tensor\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_classes = len(class_names)\n",
        "model = wasteCNN(num_classes)\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "G6rllBzMDHZx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up loss function and optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "AGnU17kbEfYe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "num_epochs = 25\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  # Each epoch has training and validaiton phase\n",
        "  for phase in ['train', 'test']:\n",
        "    if phase == 'train':\n",
        "      model.train()\n",
        "    else:\n",
        "      model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "\n",
        "    for inputs, labels in (train_loader if phase == 'train' else test_loader):\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # 1. optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 2. Forward pass\n",
        "      with torch.set_grad_enabled(phase == 'train'):\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropogation if only in training pahse\n",
        "        if phase == 'train':\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      # stats\n",
        "      running_loss += loss.item() * inputs.size(0)\n",
        "      running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss/ len(train_dataset if phase =='train' else test_dataset)\n",
        "    epoch_acc = running_corrects.double() / len(train_dataset if phase == 'train' else test_dataset)\n",
        "\n",
        "    print(f'{phase} Loss : {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "  print()\n",
        "\n",
        "print('Training Complete')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvzrPFSIGnkP",
        "outputId": "9021dd09-5021-4a2a-e9ce-4b9eb16b324b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss : 2.0962 Acc: 0.2910\n",
            "test Loss : 1.6232 Acc: 0.4143\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss : 1.4489 Acc: 0.4765\n",
            "test Loss : 1.3821 Acc: 0.5100\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss : 1.3427 Acc: 0.5238\n",
            "test Loss : 1.3028 Acc: 0.5247\n",
            "\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss : 1.2314 Acc: 0.5656\n",
            "test Loss : 1.2210 Acc: 0.5499\n",
            "\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss : 1.1689 Acc: 0.5756\n",
            "test Loss : 1.1636 Acc: 0.5773\n",
            "\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss : 1.0993 Acc: 0.5983\n",
            "test Loss : 1.1161 Acc: 0.5889\n",
            "\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss : 1.1036 Acc: 0.6069\n",
            "test Loss : 1.2090 Acc: 0.5626\n",
            "\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss : 1.0217 Acc: 0.6285\n",
            "test Loss : 1.0222 Acc: 0.6320\n",
            "\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss : 0.9680 Acc: 0.6461\n",
            "test Loss : 1.0360 Acc: 0.6246\n",
            "\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss : 0.9471 Acc: 0.6546\n",
            "test Loss : 0.9626 Acc: 0.6467\n",
            "\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss : 0.9479 Acc: 0.6609\n",
            "test Loss : 1.0146 Acc: 0.6372\n",
            "\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss : 0.8903 Acc: 0.6790\n",
            "test Loss : 0.9463 Acc: 0.6593\n",
            "\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss : 0.8472 Acc: 0.6927\n",
            "test Loss : 1.0662 Acc: 0.6383\n",
            "\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss : 0.8472 Acc: 0.6948\n",
            "test Loss : 0.9534 Acc: 0.6709\n",
            "\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss : 0.7975 Acc: 0.7106\n",
            "test Loss : 0.9612 Acc: 0.6667\n",
            "\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss : 0.7841 Acc: 0.7195\n",
            "test Loss : 0.8913 Acc: 0.6835\n",
            "\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss : 0.7991 Acc: 0.7180\n",
            "test Loss : 0.9074 Acc: 0.6751\n",
            "\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss : 0.7549 Acc: 0.7338\n",
            "test Loss : 0.8446 Acc: 0.6982\n",
            "\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss : 0.7512 Acc: 0.7293\n",
            "test Loss : 0.9245 Acc: 0.6456\n",
            "\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss : 0.7402 Acc: 0.7390\n",
            "test Loss : 0.9084 Acc: 0.6919\n",
            "\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss : 0.7288 Acc: 0.7419\n",
            "test Loss : 0.9080 Acc: 0.6793\n",
            "\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss : 0.7061 Acc: 0.7519\n",
            "test Loss : 0.8282 Acc: 0.7014\n",
            "\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss : 0.6950 Acc: 0.7580\n",
            "test Loss : 0.8292 Acc: 0.7203\n",
            "\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss : 0.6532 Acc: 0.7698\n",
            "test Loss : 0.9230 Acc: 0.6772\n",
            "\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss : 0.6637 Acc: 0.7627\n",
            "test Loss : 0.7689 Acc: 0.7245\n",
            "\n",
            "Training Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'custom_cnn_waste_classifier.pth')"
      ],
      "metadata": {
        "id": "IIzHG3EnItxF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Evaluate model"
      ],
      "metadata": {
        "id": "TdVCIxJoI2fE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the model on validation images: {100 * correct / total:.2f}%')\n",
        "\n",
        "# Inference on new images\n",
        "from PIL import Image\n",
        "\n",
        "def predict_image(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    image = data_transforms['test'](image).unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    return class_names[predicted.item()]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lKXBsuII18w",
        "outputId": "7dcaf684-4f9c-447e-f439-a90ad34c943c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on validation images: 69.30%\n"
          ]
        }
      ]
    }
  ]
}